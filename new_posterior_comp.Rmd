---
title: "A Mixture of Flexible Multivariate Distributions"
output: html_document
---

In this document, I will compute the posterior distribution for $\beta_{j}$ using the derivation in the document `next Steps', where the prior on $\beta_{j}$ is modeled as a mixture of multivariate normals.

```{r}
tuto.dir=getwd()
##' @param b.gp.hat PxR matrix of standardized effect sizes across all `R' tissue types,
##' @param se.gp.hat RxR estimated covariance matrix of standardized standard errors, corresponding to the MLE of genotype effect for a given gene-SNP pair in all tissues
##' @param t.stat PxR matrix of t statistics for each gene-snp Pair across all R tissues
##' @param U.0kl RxR prior covariance matrix for posterior.covariance matrix K and weight l
##' @param pi LxK matrix of prior weights estimated from the EM algorithm which correspond to optimal weighting of prior covariance matrix

b.gp.hat=na.omit(read.table("16008genesnppairs_43tissues_beta.hat.std.txt",header=F,skip=1)[,-c(1,2)])
se.gp.hat=na.omit(read.table("16008genesnppairs_43tissues_sigma.hat.std.txt",header=F,skip=1)[,-c(1,2)])
t.stat=na.omit(read.table("16008genesnppairs_43tissues_t.stat.txt",header=F,skip=1)[,-c(1,2)])
p.vals=na.omit(read.table("16008genesnppairs_43tissues_pval.txt",header=F,skip=1)[,-c(1,2)])

L = 2
R=ncol(b.gp.hat)#number of tissues
X.t=as.matrix(t.stat)
X.c=apply(X.t,2,function(x) x-mean(x)) ##Column centered matrix of t statistics
R=ncol(X.t)
M=nrow(X.t)
#colMeans(X.c)
```

Now, we need to load in the prior matrices which we will try to find the optimal combination. First we load in the `K' RxR prior covariance matrices specifying the relative importance of a particular tissue direction in each component. Now, for every prior covariance matrix $U_{k}$, we compute L `stretches' specifying the width of this distribution. Thus for each stretch $\omega$, there will be K corresponding covariance matrices.

```{r prior.covar}
##' @param function to return the LxK component list of prior covariance matrices
##' @param X.c a matrix of column center tstatistics
##' @param P number of PCs to keep in approximation
##' @param omega.table dataframe of stretches
##' @return return L dimensional list of K dimensional lists where each L,K contains the Lth grid component times the K covariance matrix

omega.table=read.table("~/Dropbox/cyclingstatistician/beta_gp_continuous/omega2.txt")


get.prior.covar.Ukl=function(P,L,R){
  test=list()
  for(l in 1:L){
  test[[l]]=list()
    omega=omega.table[l,]
    test[[l]][[1]]=omega*diag(1,R)# the first covariance matrix will be the 'sopped up' identity
    test[[l]][[2]]=omega*(t(X.c)%*%X.c)/M

     
  svd.X=svd(X.c)##perform SVD on sample centered
  #svd.X=svd(X.t)##peform SVD on unsample centered (same result for v)
  v=svd.X$v;u=svd.X$u;d=svd.X$d
  
  cov.pc=1/M*v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P])%*%t(v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P]))##Use the rank P summary representation  
  
  test[[l]][[3]]=omega*cov.pc}
return(U.0kl=test)}

```

Now that we have the prior covariance matrices, we can maximize the likelihood $L(\pi;\hat\beta_{j},se_{j})$ by computing $Pr(\hat \beta_{j} | Component k,l)$ for each gene componenet at each each gene SNP pair and then finding the optimal combination among all gene SNP pairs.

```{r,echo=TRUE,cache=TRUE}
##' @return Compute a likelihood using the prior covariance matrix for each gene SNP pair in the rows and componenets in the columns
##' @param b.gp.hat and se.gp.hat matrix of MLES for all J gene snp pairs
##' 
##' @param U.0kl L dimensional list of K dimensional list with prior covairnace matrix for each grid weight, prior covariance pair
#install.packages("SQUAREM") note you'll need to have SQUAREM installed
library("SQUAREM")

L=2
R=43
U.0kl=get.prior.covar.Ukl(P=25,L,R)
K=length(U.0kl[[1]])
#U.0kl[[L+1]]=diag(1,R) ##Add the identity matrix to the list 

##J = Number of gene-snp pairs to consider
##' @return likelihood a function to return a J dimensional list of KxL vectors of likelihoods for each component
library("mvtnorm")
likelihood=function(b.gp.hat,se.gp.hat,J=nrow(b.gp.hat))
  {
  lik.i=list()
lapply(seq(1:J),function(j){
  b.mle=b.gp.hat[j,]
  V.gp.hat=diag(se.gp.hat[j,])^2

   lik=matrix(NA,nrow=K,ncol=L)
 for(l in 1:L){
for (k in 1:K){
  lik[k,l]=dmvnorm(x=b.mle, sigma=U.0kl[[l]][[k]] + V.gp.hat)
  }}
   lik.i[[j]] = as.vector(lik) ## concatenates column by column (e.g., [K=1,1],[K=2,1],[K=3,1],[2,1])


})}

##Likelihood of each component in cols, gene=snp pairs in rows##
global.lik=matrix(unlist(likelihood(b.gp.hat,se.gp.hat)),ncol=K*L,byrow=TRUE)



```
Now, to use the EM algorithm:

```{r, echo=FALSE,cache=TRUE}
 #@details Fits a k component mixture model \deqn{f(x|\pi) = \sum_k \pi_k f_k(x)} to independent
#' and identically distributed data \eqn{x_1,\dots,x_n}. 
#' Estimates posterior on mixture proportions \eqn{\pi} by Variational Bayes, 
#' with a Dirichlet prior on \eqn{\pi}. 
#' Algorithm adapted from Bishop (2009), Pattern Recognition and Machine Learning, Chapter 10.
#' 
#' @param matrix_lik a n by k matrix with (j,k)th element equal to \eqn{f_k(x_j)}.
#' @param prior a k vector of the parameters of the Dirichlet prior on \eqn{\pi}. Recommended to be rep(1,k)
#' @param pi.init the initial value of the posterior parameters. If not specified defaults to the prior parameters.
#' @param control A list of control parameters for the SQUAREM algorithm, default value is set to be   control.default=list(K = 1, method=3, square=TRUE, step.min0=1, step.max0=1, mstep=4, kr=1, objfn.inc=1,tol=1.e-07, maxiter=5000, trace=FALSE). 
#' 
#' @return A list, whose components include point estimates (pihat), 
#' the parameters of the fitted posterior on \eqn{\pi} (pipost),
#' the bound on the log likelihood for each iteration (B)
#' and a flag to indicate convergence (converged).
#'  
#' @export
#' 

library(SQUAREM)
mixEM = function(matrix_lik,prior,pi.init=NULL,control=list()){
  control.default=list(K = 1, method=3, square=TRUE, step.min0=1, step.max0=1, mstep=4, kr=1, objfn.inc=1,tol=1.e-07, maxiter=5000, trace=FALSE)
  namc=names(control)
  if (!all(namc %in% names(control.default))) 
    stop("unknown names in control: ", namc[!(namc %in% names(control.default))])
  controlinput=modifyList(control.default, control)
  
  k=dim(matrix_lik)[2]
  if(is.null(pi.init)){
    pi.init = rep(1/k,k)# Use as starting point for pi
  } 
  res = squarem(par=pi.init,fixptfn=fixpoint, objfn=negpenloglik,matrix_lik=matrix_lik, prior=prior, control=controlinput)
  return(list(pihat = normalize(pmax(0,res$par)), B=res$value.objfn, 
              niter = res$iter, converged=res$convergence))
}

# helper functions used by mixEM
normalize = function(x){return(x/sum(x))}

fixpoint = function(pi, matrix_lik, prior){  
  pi = normalize(pmax(0,pi)) #avoid occasional problems with negative pis due to rounding
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k
  m.rowsum = rowSums(m)
  classprob = m/m.rowsum #an n by k matrix
  pinew = normalize(colSums(classprob) + prior - 1)
  return(pinew)
}

negpenloglik = function(pi,matrix_lik,prior){return(-penloglik(pi,matrix_lik,prior))}

penloglik = function(pi, matrix_lik, prior){
  pi = normalize(pmax(0,pi))
  m  = t(pi * t(matrix_lik)) # matrix_lik is n by k; so this is also n by k
  m.rowsum = rowSums(m)
  loglik = sum(log(m.rowsum))
  subset = (prior != 1.0)
  priordens = sum((prior-1)[subset]*log(pi[subset]))
  return(loglik+priordens)
}

#The kth element of this vector is the derivative 
#of the loglik for $\pi=(\pi_0,...,1-\pi_0,...)$ with respect to $\pi_0$ at $\pi_0=1$.
gradient = function(matrix_lik){
  n = nrow(matrix_lik)
  grad = n - colSums(matrix_lik/matrix_lik[,1]) 
  return(grad)
}
```

```{r}
#install.packages("SQUAREM")
library("SQUAREM")
pis=mixEM(matrix_lik=global.lik,prior=rep(1,K*L))
names.vec=matrix(NA,nrow=K,ncol=L)
  for(l in 1:L){ 
    for(k in 1:K){
      names.vec[k,l]=paste0("k=",k,";l=",l)}}

write.table((cbind(as.vector(names.vec),pis$pihat)),quote=FALSE,file="piklhat.txt")
pi.hat=read.table(file="piklhat.txt")
barplot(pis$pihat,names=as.vector(names.vec),main="mixEM estimated pi")
```

We can see that the majority of the weight is put on the covariance matrix of t statistics, $X_{t}'X$ and the estimated covariance matrix, $V_{t,1..K}\lambda^{2} V_{t,1..K}'$ where here I use K = 13. Recall each V is a $43$ x $1$ vector. 
 
I compare with naiive iteration, which simply weights the relative likelihood across individuals.

```{r,cache=TRUE}
##' @param global.lik Computes the likelihood matrices for each componenet for each of j gene snp pairs
##' @return global.sum Sums the likelihood for each component across j gene snp pairs
##' @return global.norm Sums the likelihood for all components across all j pairs


library("mvtnorm")

updated.weights=function(b.gp.hat,se.gp.hat,global.lik){
    global.sums=as.matrix(colSums(global.lik))#relative importance of each componenet across all gene-snp pairs
  global.norm=sum(global.sums)
return(updated.weights=global.sums/global.norm)}

names.vec=matrix(NA,nrow=K,ncol=L)
  for(l in 1:L){ 
    for(k in 1:K){
      names.vec[k,l]=paste0("k=",k,";l=",l)}}

x=updated.weights(b.gp.hat,se.gp.hat,global.lik)
barplot(as.vector(x),names=as.vector(names.vec),main="Normalized Likelihood at Each Component")


```
Here, I've plotted the relative importance of each component after one iteration with a uniform prior weight on each $\pi$.

Recall:

$$L(\beta_{j};k,l) = Pr(\hat{b}_{j} | k,l)$$
$$=Pr(\hat{b}_{j}; 0, \omega^{2}_{l} U_{k} + \hat {V})$$



$$w_{jkl} = \frac{\pi_{k,l} L(\beta_{j};k,l)}{\sum_{k,l} \pi_{k,l} L(\beta_{j};k,l)}$$


We can then update our estimate of $\pi_{k,l}$ 


$$\pi_{kl}^{i+1} = \frac{\sum_{j} w_{jkl}}{\sum_{j,k,l} w_{jkl}}$$

##Posterior computation#

Now that we have the hierarchical weights $\pi_{k,l}$, we can compute the posterior distribution for each component.

For a given prior covariance matrix, compute posterior covariance and posterior mean. Here, let U.0k.l represent a specific matrix in U.0kl (.e.g, $U.0kl[[l]][[k]]$)

```{r}
##' @param U.0k.l let U.0k.l represent a specific matrix in U.0kl (.e.g, U.0kl[[l]][[k]])
##' @return post.b.gpkl.cov ## returns an R*R posterior covariance matrix for jth gene snp pair
##' @return post.b.gpkl.mean return a 1 * R vector of posterior means for a given prior covariance matrix

post.b.gpkl.cov <- function(V.gp.hat.inv, U.0k.l){
        U.gp1kl <- U.0k.l %*% solve(V.gp.hat.inv %*% U.0k.l + diag(nrow(U.0k.l)))
            return(U.gp1kl)
    }

post.b.gpkl.mean <- function(b.mle, V.gp.hat.inv, U.gp1kl){
        mu.gp1kl <- U.gp1kl %*% V.gp.hat.inv %*% b.mle
            return(mu.gp1kl)
    }
```

We also need to compute the "posterior weights" corresponding to each prior covariance matrix, which is simply the likelihood evaluated at that componenet times the prior weigth, $pi_{k,l}$ normalized by the marginal likelihood over all components.

$p(k=1,l=1|D)=$
$\frac{p(D|k=1,l=1)*p(k=1,l=1)}{p(D)}$

```{r}
##' @param pi.hat = matrix of prior weights
##' @return a vector of posterior weights
pis=pi.hat[,2]
post.weight.func=function(pis,U.0kl,V.gp.hat,b.mle){
post.weight.num = matrix(NA,nrow=K,ncol=L)
for(k in 1:K){
for(l in 1:L){
  wts=matrix(pis,nrow=3,ncol=2)
  pi=wts[k,l]
  post.weight.num[k,l]=pi*dmvnorm(x=b.mle, sigma=U.0kl[[l]][[k]] + V.gp.hat)}
}
post.weight=post.weight.num/sum(post.weight.num)
return(as.vector(post.weight))}

```

Now, for each gene-snp pair $j$ and each prior covariance matrix $U_{0kl}$ I will generate a 43 x 43 posterior covariance matrix and 43 x 1 vector of posterior means.

```{r postmeanandcovars, cache=TRUE}
##' @param U.0kl = l dimensional list of k dimensional list of prior covariance matrices
##' @returm all.means, a J dimensional list of k dimensional lists of L dimensional list of 1xR vectors of posterior means
##' @returm all.covs, a J dimensional list of k dimensional lists of L dimensional list of RxR posterior covariances
##' @returm post.weight.matrix a J x (K*L) matrix of posterior weights coresponding to p(K,L|D) for each gene-snp Pair


all.covs=list()
all.means=list()
J=dim(b.gp.hat)[1]
#lapply(seq(1:J),function(j){
  for(j in 1:J){
    b.mle=as.vector(t(b.gp.hat[j,]))##turn i into a 43 x 1 vector
    V.gp.hat=diag(se.gp.hat[j,])^2
    all.covs[[j]]=list()
    all.means[[j]]=list()
    temp.mean=matrix(NA,nrow=)

            V.gp.hat.inv <- solve(V.gp.hat)
          for(k in 1:K){
            all.covs[[j]][[k]]=list()
            all.means[[j]][[k]]=list()
            for (l in 1:L){
                  all.means[[j]][[k]][[l]]=list()
                    all.covs[[j]][[k]][[l]]=list()
                  U.gp1kl <- post.b.gpkl.cov(V.gp.hat.inv, U.0kl[[l]][[k]]) ## returns an R*R posterior covariance matrix for jth gene snp pair
                  mu.gp1kl <- post.b.gpkl.mean(b.mle, V.gp.hat.inv, U.gp1kl) ##return a 1 * R vector of posterior means for a given prior covariance matrix
                  all.means[[j]][[k]][[l]]=mu.gp1kl
                  all.covs[[j]][[k]][[l]]=U.gp1kl
                      }
           }}

```

Now, for each of the $J$ gene-snp pairs we generate a matrix of posterior weight matrix. Remember, this will not be tissue specific information, so we need to do it only once and can store in an Jx(K*L) matrix

```{r postweightmatrix,cache=TRUE}
library("mvtnorm")
post.weight.matrix=matrix(NA,nrow=J,ncol=L*K)
J=dim(b.gp.hat)[1]


for(j in 1:J){
    b.mle=as.vector(t(b.gp.hat[j,]))##turn i into a 43 x 1 vector
    V.gp.hat=diag(se.gp.hat[j,])^2
    pis=pi.hat[,2]
    U.0kl=U.0kl
    post.weight.matrix[j,]=as.vector(post.weight.func(pis,U.0kl,V.gp.hat,b.mle))
}

```


```{r,echo=FALSE,eval=TRUE,cache=TRUE}

files=list.files(path="/Users/sarahurbut/Dropbox/cyclingstatistician/filtered/",pattern="_filter.txt")
names=NULL
      for(i in 1:length(files)){
            a=strsplit(files[i], '[.]')[[1]][1]
                                 names[i]=(strsplit(a, '[_]')[[1]][1])
                                                       }
```
We can compute the overall posterior mean for each gene-SNP pair across tissues (i.e., the weighted $R$ dimensional posterior mean vector of $mu_{j}$). Here, I do it for 10 gene SNP pairs.
```{r,cache=TRUE}
J=10
for(j in 1:J){
post.means=matrix(NA,nrow=J,ncol=R)
mean.list=all.means[[j]]
post.weights=post.weight.matrix[j,]
post.weight.mat=matrix(post.weights,nrow=3,ncol=2)
post.weight.mat[is.na(post.weight.mat)] <- 0

temp=NULL
for(k in 1:K){
for(l in 1:L){
temp=rbind(as.vector(post.weight.mat[k,l]*mean.list[[k]][[l]]),temp)##creates a 6x43 matrix of posterior means, but in [k=3,l=2][k=3,l=1][k=2,l=2][k=2,l=1] etc. (which is ok because we simply sum)
}
}
post.means[j,]=t(colSums(temp))
#pdf(paste0("PostTissueMeans,B_",j))
par(mfrow=c(1,2))
barplot(post.means[j,],main=paste0("PostTissueMeans,B_",j),col=c(1:43),las=2,names=names[-44])
#text(post.means[j,],labels=names, srt=45, pos=1,col=c(1:43))
b.mle=t(b.gp.hat[j,])
plot(b.mle,post.means[j,],main="PostMeanvsMLE")
abline(0,1)##concern because the order of magnitude diff, but prior weight heavy on (U.0kl[[2]][[3]]) which has very small avg. variance
#dev.off()

}
```

In order to compute the factors, we perform the following analysis.

1) We perfrom the factor decomposition and use the first $K$ = 10 matrix approximations to estimate the covariance of X^{t}X. HOwever, rather than add them as we did in PCs, we compute the weights for each componenet separately so that we can quantify the weight on the each 'Factor Approximation' of the covariance. Recall that the loading corresponding to factor 1 corresponds to how much of 'master tissue pattern1' each tissue carries, and so the entry for that tissue in the $kth$ covariance matrix approximation will tell you the size of the effect in the direction of this 'master tissue-k specific pattern'. Similarly, the off-diagonal elements tell us the covariance between the performnace of tissues $i$ and $j$ in this direction. See my document section entitled 'Interpretation.' I also use the full rank $K=10$ approximation.

```{r, eval=FALSE}
sfa -gen ./centered.t.stats.txt -t -g 43 -n 6904 -o tri -k 18
```
Then, I will use $K=18$ factor representations of the covariance matrix of $X^{t}X$.
```{r using Factors}
library("mvtnorm")
lambda=as.matrix(read.table("tri_lambda.out"))
factor=as.matrix(read.table("tri_F.out"))
P=25
get.prior.covar.Ukl=function(P,L,R,F=18){
  test=list()
  for(l in 1:L){
  test[[l]]=list()
    omega=omega.table[l,]
    test[[l]][[1]]=omega*diag(1,R)# the first covariance matrix will be the 'sopped up' identity
    test[[l]][[2]]=omega*(t(X.c)%*%X.c)/M

     
  svd.X=svd(X.c)##perform SVD on sample centered
  #svd.X=svd(X.t)##peform SCD on unsample centered (same result for v)
  v=svd.X$v;u=svd.X$u;d=svd.X$d
  
    cov.pc=1/M*v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P])%*%t(v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P]))##Use the rank P summary representation  


  #cov.pc=1/M*v[,1:P]%*%diag(d[1:P])%*%t(v[,1:P])##Use the rank P summary representation  
  
  test[[l]][[3]]=omega*cov.pc
  for(f in 1:F){
    load=as.matrix(lambda[,f])
    fact=t(as.matrix(factor[f,]))
    test[[l]][[f+3]]=1/M*(load%*%fact)%*%t((load%*%fact))
    }
  test[[l]][[F+4]]=1/M*(lambda%*%factor)%*%t((lambda%*%factor))}##Note that this is NOT equivalent to the original covariance matrix because  the maximum number of cactors is limited to 10
  
return(U.0kl=test)}

U.0kl=get.prior.covar.Ukl(P=25,L=3,R=43,F=18)
K=length(U.0kl[[1]])
##Now, compute the additional likielihoods
global.lik=matrix(unlist(likelihood(b.gp.hat,se.gp.hat)),ncol=K*L,byrow=TRUE)
pis2=mixEM(matrix_lik=global.lik,prior=rep(1,K*L))
names.vec=matrix(NA,nrow=K,ncol=L)
  for(l in 1:L){ 
    for(k in 1:K){
      names.vec[k,l]=paste0("k=",k,";l=",l)}}

#write.table((cbind(as.vector(names.vec),pis$pihat)),quote=FALSE,file="piklhat.txt")
#pi.hat=read.table(file="piklhat.txt")
par(mfrow=c(2,1))
barplot(pis2$pihat,main="mixEM estimated pi",names=names.vec,las=2,col=c(1:14))


x=updated.weights(b.gp.hat,se.gp.hat,global.lik)
barplot(as.vector(x),main="naiiveWeights estimated pi",names=names.vec,las=2,col=c(1:14))
```



